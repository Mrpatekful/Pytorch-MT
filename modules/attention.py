import torch
from torch import nn


class Attention(nn.Module):

    def __init__(self):
        super(Attention, self).__init__()

    def forward(self):
        return 0
